<html>

<head>
	<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"> </script>
	<script src="mobilenet.js"> </script>


</head>

<body>
	<div>
		<label id="className" style="font-size:50px;"> </label>
	</div>
	<video id="myVidPlayer" controls muted autoplay></video>

	<canvas width=224 height=224 id="output"></canvas>

	<script>

		var canvas = document.getElementById("output");
		var context = canvas.getContext("2d");
		const video = document.querySelector('#myVidPlayer');

		//w-width,h-height
		var w, h;
		canvas.style.display = "none";


		window.navigator.mediaDevices.getUserMedia({ video: true }).
			then(stream => {
				video.srcObject = stream;
				video.onloadedmetadata = e => {
					video.play();

					//new
					w = video.videoWidth;
					h = video.videoHeight;

					canvas.width = w;
					canvas.height = h;
				};
			}).
			catch(error => {
				alert('You have to enable the mike and the camera');
			});



		// process video
		async function video_tagging() {

			// get video stream
			const stream = document.getElementById("myVidPlayer");

			// load and prepare HTML canvas
			const canvas = document.getElementById("output");
			var draw = canvas.getContext("2d");
			draw.font = "20px Arial";
			draw.fillStyle = "yellow";

			// load the mobile-net model
			const model = await mobilenet.load();
			// const model = await tf.loadGraphModel('tfjs_model_mobilenetv3/model.json');
			// const model = await tf.loadLayersModel('tfjs_model_mobilenetv3/model.json');

			while (1) {

				// fill canvas object with video stream
				draw.drawImage(stream, 0, 0, canvas.width, canvas.height);

				const result = await model.classify(canvas);

				const class_label = result[0].className;

				document.getElementById('className').innerHTML = class_label;

				await tf.nextFrame();
			}

		}


		// check if the video is ready for processing
		function main() {
			video_tagging();
		}

		main();


	</script>




</body>

</html>